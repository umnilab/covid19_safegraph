{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make City Data\n",
    "Aggregate data of cities (metropolitan regions, given by a collection of counties) from the county data stored as per `make_county_data.ipynb`.\n",
    "\n",
    "**File structure**:\n",
    "- `~/city_wise/`\n",
    "    - `<city_name>/`\n",
    "        - `shapefile/`\n",
    "        - `places.pickle`\n",
    "        - `census.pickle`\n",
    "        - `patterns_<start date>_<end date>.pickle`\n",
    "        - `homes_<start date>_<end date>.pickle`\n",
    "        - `social_dist_<start date>_<end date>.pickle`\n",
    "        - `social_od_<start date>_<end date>.pickle`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from covid_commons import *\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import geopandas as gp\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from time import time\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import covid_commons as g\n",
    "from covid_commons import peek\n",
    "g = reload(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class City():\n",
    "    def __init__(self, key, dict_):\n",
    "        self.key = key  # key in the cities dictionary\n",
    "        self.name = dict_['name']\n",
    "        self.pretty_name = self.name.replace('_', ' ').title()\n",
    "        self.dir = IO['city_root'] + '/' + self.name\n",
    "        self.counties = dict_['counties']\n",
    "        self.events = dict_['events']\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'<City:{self.name}>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(IO['city_info'], 'r') as f:\n",
    "    cities_dict = json.load(f)\n",
    "    C = {k: City(k, v) for k, v in cities_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illinois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = g.City('il', {\n",
    "    'name': 'Illinois',\n",
    "    'events': {},\n",
    "    'counties': {x[-3:]: [17, int(x[-3:])] for x in\n",
    "                 glob.glob(g.DATA_DIR+'/county_wise/17/*')}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = {'il': il}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load city data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_city_data(city, variable, ftype='pickle', dates=None, cbg_var=False):\n",
    "    \"\"\"\n",
    "    Load the data of a given variable in the given city & period.\n",
    "    @param city: <City>\n",
    "    @param dates: <pd.DateTimeIndex> dates for which data is to be retrieved\n",
    "    @param variable: <str> measure of interest\n",
    "    @param cbg_var: <bool>\n",
    "    @param ftype: <str> extension of data file (one of 'pickle' or 'csv')\n",
    "    @return data: <pd.df>\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame()\n",
    "    for cnty_name, (state, cnty) in tqdm(city.counties.items(), desc=city.name):\n",
    "        dir_ = f'{IO[\"cnty_root\"]}/{state:02}/{cnty:03}'\n",
    "        # when the content is static\n",
    "        if dates is None:\n",
    "            file = f'{dir_}/{variable}.{ftype}'\n",
    "            if os.path.exists(file):\n",
    "                if ftype == 'pickle':\n",
    "                    data = data.append(pd.read_pickle(file), ignore_index=True)\n",
    "                elif ftype == 'csv':\n",
    "                    df = pd.read_csv(file)\n",
    "                    df.insert(0, 'state', state)\n",
    "                    df.insert(1, 'cnty', cnty)\n",
    "                    data = data.append(df)\n",
    "        # when the content is dynamic (daily/weekly)\n",
    "        else:\n",
    "            for date in dates:\n",
    "                date_str = date.strftime('%Y-%m-%d')\n",
    "                file = f'{dir_}/{variable}/{variable}_{date_str}.pickle'\n",
    "                if os.path.exists(file):\n",
    "                    df = pd.read_pickle(file)\n",
    "                    df['date'] = int(date.strftime('%y%m%d'))\n",
    "                    if cbg_var == False:\n",
    "                        df['state'] = state\n",
    "                        df['cnty'] = cnty\n",
    "                        df = df.astype({'state': np.int8, 'cnty': np.int16})\n",
    "                    df = df.astype({'date': np.int32})\n",
    "                    data = data.append(df, ignore_index=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save city data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_city_data(data, city, fname, ftype='pickle', dates=None):\n",
    "    \"\"\"\n",
    "    Write the combined data of city's counties to disk\n",
    "    \"\"\"\n",
    "    if not os.path.exists(city.dir):\n",
    "        os.makedirs(city.dir)\n",
    "    if dates is None:\n",
    "        file = f'{city.dir}/{fname}.{ftype}'\n",
    "    else:\n",
    "        file = f'{city.dir}/{fname}_{dateRange2str(dates)}.{ftype}'\n",
    "    if ftype == 'pickle':\n",
    "        data.to_pickle(file)\n",
    "    elif ftype == 'csv':\n",
    "        data.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw tabular datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $R_t$ (health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1305dd515c4fdeae8aa5bca3f903dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Illinois', max=102.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for c in C.values():\n",
    "    c.rt = load_city_data(c, 'rt', 'csv')\n",
    "    save_city_data(c.rt, c, 'rt', 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3d099d8cfd483f91572d0c994703b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Illinois', max=102.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for c in C.values():\n",
    "    c.pois = load_city_data(c, 'places')\n",
    "    save_city_data(c.pois, c, 'places')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d967d5c8399d4671867c268f5169a8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Illinois', max=102.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for c in C.values():\n",
    "    c.acs = load_city_data(c, 'census')\n",
    "    save_city_data(c.acs, c, 'census')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ceecf1035442e5bda6935fd59d1373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Illinois', max=102.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 36min 40s, sys: 3min 31s, total: 40min 11s\n",
      "Wall time: 42min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for c in C.values():\n",
    "    save_city_data(load_city_data(c, 'patterns', dates=g.WEEKS),\n",
    "                   c, 'patterns', dates=g.WEEKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly pattern OD\n",
    "Origin-destination table of no. of visitors between home CBG & POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732af54063d14a72b870fcc8fe211f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Illinois', max=102.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 8min 20s, sys: 5min 49s, total: 14min 10s\n",
      "Wall time: 14min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for c in C.values():\n",
    "    save_city_data(load_city_data(c, 'homes', dates=g.WEEKS),\n",
    "                   c, 'patterns_od', dates=g.WEEKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social distancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5cb0cbcb922426f900516e46fafaec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Illinois', max=102.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 37min 12s, sys: 11min 31s, total: 48min 44s\n",
      "Wall time: 52min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for c in C.values():\n",
    "    save_city_data(load_city_data(c, 'social_dist', dates=g.DATES),\n",
    "                   c, 'social_dist', dates=g.DATES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social distancing OD\n",
    "Origin-destination table of no. of devices moving from home CBG to non-home CBG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e60aec521d476c9ac877ee3e2bdf24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Illinois', max=102.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 3h 24min 26s, sys: 3h 3min 58s, total: 6h 28min 25s\n",
      "Wall time: 6h 54min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for c in C.values():\n",
    "    save_city_data(load_city_data(c, 'social_od', dates=g.DATES),\n",
    "                   c, 'social_od', dates=g.DATES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBG Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cbg_shapefile(city, write=True):\n",
    "    \"\"\"\n",
    "    Extract the shapefile of the given city from its state shapefile\n",
    "    containing the CBGs & save it to the city's folder.\n",
    "    \"\"\"\n",
    "    # get the shapefile by filtering the counties\n",
    "    shp = []\n",
    "    for state_fips, cnty_fips in city.counties.values():\n",
    "        shp_file = g.IO['state_shp'].format(state_fips)\n",
    "        df = gp.read_file(shp_file)\n",
    "        shp.append(df[df['COUNTYFP'] == f'{cnty_fips:03}'])\n",
    "    shp = pd.concat(shp, axis=0, ignore_index=True)\n",
    "    \n",
    "    # save to disk\n",
    "    if write:\n",
    "        dir_ = f'{city.dir}/shapefile'\n",
    "        if not os.path.exists(dir_):\n",
    "            os.makedirs(dir_)\n",
    "        file = f'{dir_}/{city.name}_CBG.shp'\n",
    "        shp.to_file(file)\n",
    "    \n",
    "    return shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pbar = tqdm(C.values())\n",
    "for c in pbar:\n",
    "    pbar.set_description(c.name)\n",
    "    c.shp = get_cbg_shapefile(c, write=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map(city, col, colname=None, cmap='Reds',\n",
    "             min_=None, max_=None, figsize=(10, 10)):\n",
    "    \"\"\"\n",
    "    Plot the thematic map of the CBGs of a city by a continuous variable.\n",
    "    @param city: <City> city of interest\n",
    "    @param col: <str> census variable/column of interest (continuous)\n",
    "    @param colname: <str> pretty name of the census variable\n",
    "    @param min_, max_: <float> lower and upper limits on the colorbar\n",
    "    \"\"\"\n",
    "    # join the shapefile with the census field by CBG\n",
    "    gdf = (city.shp.astype({'GEOID': np.int64})\n",
    "           .merge(city.acs.set_index('cbg')[col].dropna(),\n",
    "                  left_on='GEOID', right_index=True))\n",
    "    \n",
    "    # cut off extreme values to prevent colormap from exploding\n",
    "    extend = 'neither'\n",
    "    if max_ is not None:\n",
    "        gdf.loc[gdf[col] > max_, col] = max_\n",
    "        extend = 'max'\n",
    "    if min_ is not None:\n",
    "        gdf.loc[gdf[col] < min_, col] = min_\n",
    "        extend = 'min'\n",
    "    if max_ is not None and min_ is not None:\n",
    "        extend = 'both'\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # create colorbar legend\n",
    "    vmin, vmax = gdf[col].min(), gdf[col].max()\n",
    "    sm = plt.cm.ScalarMappable(\n",
    "        cmap=cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='2%', pad=0.05)\n",
    "    plt.colorbar(sm, cax=cax, extend=extend)\n",
    "    \n",
    "    # main plot\n",
    "    gdf.plot(ax=ax, column=col, cmap=cmap, legend=False)\n",
    "    colname = colname if colname is not None else col\n",
    "    ax.set_title(f'CBGs by {colname} (2017)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time plot_map(C['nyc'], 'med_hh_income', cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### County shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnty_shapefile(city, write=False):\n",
    "    \"\"\"\n",
    "    Extract the shapefile of the counties of a city & save if required.\n",
    "    \"\"\"\n",
    "    # specify how columns are to be aggregated\n",
    "    agg = {'STATEFP': min, 'COUNTYFP': min, 'ALAND': sum, 'AWATER': sum}\n",
    "    \n",
    "    # get the shapefile by filtering the counties\n",
    "    records = []\n",
    "    geometries = []\n",
    "    for state_fips, cnty_fips in city.counties.values():\n",
    "        shp_file = g.IO['state_shp'].format(state_fips)\n",
    "        state_df = gp.read_file(shp_file)\n",
    "        # get the county level dataframe\n",
    "        df = state_df.query(f'COUNTYFP == \"{cnty_fips:03}\"')\n",
    "        # filter out the all water CBGs\n",
    "        df = df.query('ALAND > 0')\n",
    "        # compute its unary union (time consuming step)\n",
    "        union = df['geometry'].unary_union\n",
    "        geometries.append(union)\n",
    "        # aggregate other attributes\n",
    "        records.append(df.agg(agg))\n",
    "    # create the pandas and geopandas dataframes\n",
    "    records = pd.DataFrame(records)\n",
    "    shp = gp.GeoDataFrame(gp.GeoSeries(geometries).rename('geometry'))\n",
    "    for col in agg.keys():\n",
    "        shp[col] = records[col]\n",
    "    \n",
    "    # save to disk\n",
    "    if write:\n",
    "        dir_ = f'{city.dir}/shapefile'\n",
    "        if not os.path.exists(dir_):\n",
    "            os.makedirs(dir_)\n",
    "        file = f'{dir_}/{city.name}_cnty.shp'\n",
    "        shp.to_file(file)\n",
    "    \n",
    "    return shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pbar = tqdm(C.values())\n",
    "for c in pbar:\n",
    "    pbar.set_description(c.name)\n",
    "    c.shp = get_cnty_shapefile(c, write=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exposure mobility metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_pat_poi(city):\n",
    "    \"\"\"\n",
    "    Load the patterns data of a city and isolate its hourly visits matrix\n",
    "    \"\"\"\n",
    "    # get the patterns data\n",
    "    if not hasattr(city, 'pat') or not 'visits_hourly' in city.pat.columns:\n",
    "        pat = pd.read_pickle(f'{city.dir}/patterns_{dateRange2str(WEEKS)}.pickle')\n",
    "        # join with POI table to get floor area\n",
    "        pois = (city.pois[['poi_id', 'naics', 'includes_parking_lot', 'area_sqft']]\n",
    "                .rename(columns={'area_sqft': 'area'}))\n",
    "        city.pat = pat.merge(pois, on='poi_id')\n",
    "        \n",
    "    # get the hourly visits matrix\n",
    "    if not hasattr(city, 'poi_visits_hourly'):\n",
    "        # pop the column to not occupy more space\n",
    "        vis = city.pat.pop('visits_hourly')\n",
    "        # convert to matrix\n",
    "        vis = np.reshape(np.stack(vis), (vis.shape[0]*7, 24))\n",
    "        # convert week index to date & format it\n",
    "        days = (pd.to_datetime(np.repeat(city.pat['date'], 7), format='%y%m%d') +\n",
    "                pd.to_timedelta(np.tile(np.arange(7), city.pat.shape[0]), unit='d'))\n",
    "        # prepare the table\n",
    "        city.visits_hourly = pd.DataFrame(vis, index=days).rename_axis('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the exposure metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_exp_mob(city, midpoints=g.DWELL_BINS['exp_hour'], include_last_bin=True,\n",
    "                write=False):\n",
    "    \"\"\"\n",
    "    Get the POI-level daily exposure based mobility metrics of a city\n",
    "    from hourly POI visits, areas, and weekly dwell time distribution matrix.\n",
    "    @param midpoints: <[]> representative points of required dwell time buckets\n",
    "    @param include_last_bin: whether count the visits of the last bucket\n",
    "    (>4 hr) as valid visits >1 h. When false, combine visits of bins 1-4 hr &\n",
    "    >4 hr into one category with representative point of 1 hr\n",
    "    \"\"\"\n",
    "    # get the patterns data (row: (poi, week))\n",
    "    pat = city.pat\n",
    "\n",
    "    # get index of POIs to be filtered out because their area includes parking lot\n",
    "    idx = pat['includes_parking_lot'] == False\n",
    "\n",
    "    # get the area of these filtered POIs\n",
    "    areas = np.repeat(pat[idx]['area'].values, 7)\n",
    "\n",
    "    # get the hourly visits dataframe & convert it to matrix\n",
    "    viz_df = city.visits_hourly\n",
    "    viz_mat = viz_df.values[np.repeat(idx.values, 7), :]\n",
    "\n",
    "    # convert ones to zeros in this matrix because ones, like zeros, have no\n",
    "    # contribution in social contact; converting them to zero ensures that\n",
    "    # those ones do not contribute to the hourly social time becoming -ve\n",
    "    viz_mat = np.where(viz_mat == 1, 0, viz_mat)\n",
    "\n",
    "    # get the daily visits\n",
    "    daily_viz = viz_mat.sum(axis=1)\n",
    "\n",
    "    # calculate the daily RPS\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        rps = np.sqrt(viz_mat.sum(1) * np.sqrt(areas) / daily_viz)\n",
    "\n",
    "    # get the weekly dwell time distribution matrix for PET\n",
    "    distr = np.array(pat[idx]['dwell_bins'].tolist()).copy()\n",
    "    distr = distr / distr.sum(axis=1)[:, None]\n",
    "    # if required, add visits of 1-4 hr & >4 hr with representative point =1 hr\n",
    "    if include_last_bin:\n",
    "        distr[:, 3] = distr[:, 3] + distr[:, 4]\n",
    "    distr = distr[:, :4]\n",
    "    d1, d2, d3, d4 = distr.T\n",
    "    t1, t2, t3, t4 = midpoints\n",
    "\n",
    "    # compute the weekly alpha coefficient (see the PET formula)\n",
    "    alpha = (t1 * (d1 ** 2 + 2 * d1 * d2 + 2 * d1 * d3 + 2 * d1 * d4) +\n",
    "             t2 * (d2 ** 2 + 2 * d2 * d3 + 2 * d3 * d4) +\n",
    "             t3 * (d3 ** 2 + 2 * d3 * d4) +\n",
    "             t4 * (d4 ** 2))\n",
    "\n",
    "    # calculate the weekly beta coefficient (see the PET formula)\n",
    "    beta = distr @ midpoints\n",
    "\n",
    "    # apply the main formula of contact time to get the daily averaged social\n",
    "    # time, keeping in mind that weekly vectors are cast to daily by repeating\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        pet = np.repeat(alpha, 7) * ((viz_mat ** 2).sum(1) / (\n",
    "            daily_viz)) - np.repeat(beta, 7)\n",
    "    # clip the rare negative values to 0\n",
    "    pet = np.clip(pet, a_min=0, a_max=None)\n",
    "\n",
    "    # calculate the POI-daily contact exposure index (CEI)\n",
    "    # unit: minute-persons per foot\n",
    "    cei = pet / np.sqrt(areas)\n",
    "\n",
    "    # create the POI-daily exposure dataframe\n",
    "    exp_df = (\n",
    "        pd.DataFrame({\n",
    "            'date': viz_df.index.rename('date')[np.repeat(idx.values, 7)],\n",
    "            'cbg': np.repeat(pat[idx]['cbg'].values, 7),\n",
    "            'poi_id': np.repeat(pat[idx]['poi_id'].values, 7),\n",
    "            'naics': np.repeat(pat[idx]['naics'].values, 7),\n",
    "            'area': areas,\n",
    "            'visits': daily_viz,\n",
    "            'rps': rps,\n",
    "            'pet': pet,\n",
    "            'cei': cei\n",
    "        })\n",
    "        .dropna()\n",
    "        .astype({'visits': np.uint16, 'area': np.int32, 'poi_id': np.int32})\n",
    "        .set_index(['date', 'cbg', 'poi_id', 'naics'])\n",
    "    )\n",
    "    if write:\n",
    "        if include_last_bin:\n",
    "            exp_df.to_pickle(city.dir + '/exposure.pickle')\n",
    "        else:\n",
    "            exp_df.to_pickle(city.dir + '/exposure4.pickle')\n",
    "    return exp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate exposure mobility vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def agg_exp(city, grp_vars, exp_vars=['rps', 'pet', 'cei'], fname=None):\n",
    "    \"\"\"\n",
    "    Aggregate exposure metrics over a given scope as averages weighted by\n",
    "    visits in that scope. `grp_vars` defines that scope. Optionally, also\n",
    "    write the output dataframe to disk by the `fname`.\n",
    "    \"\"\"\n",
    "    df = city.exp.dropna()\n",
    "    for var in exp_vars:\n",
    "        df[var+'_x_visits'] = df['visits'] * df[var]\n",
    "    agg_df = (df.groupby(grp_vars)\n",
    "              [['visits'] + [x+'_x_visits' for x in exp_vars]].sum())\n",
    "    for var in exp_vars:\n",
    "        agg_df[var] = agg_df[var+'_x_visits'] / agg_df['visits']\n",
    "        del agg_df[var+'_x_visits']\n",
    "\n",
    "    if fname is not None:\n",
    "        agg_df.to_pickle(f'{city.dir}/{fname}.pickle')\n",
    "    \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate for all cities\n",
    "* Over CBGs\n",
    "* Over industries (NAICS)\n",
    "* Over both CBGs & NAICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53adee0a9a04207907d54f0e27439b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 7.78 s, sys: 2.76 s, total: 10.5 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for c in tqdm(C.values()):\n",
    "    t = time()\n",
    "#     print('-'*20+'\\n', c.name)\n",
    "#     print('getting pat poi')\n",
    "#     c.pois = pd.read_pickle(c.dir + '/places.pickle')\n",
    "#     get_pat_poi(c)\n",
    "#     print('getting exposure metrics')\n",
    "#     c.exp = get_exp_mob(c, write=True)\n",
    "#     c.exp = pd.read_pickle(c.dir + '/exposure.pickle')\n",
    "#     print('getting exposure metrics with 4 bins')\n",
    "#     c.exp4 = get_exp_mob(c, write=True, include_last_bin=False)\n",
    "#     print('aggregating by CBG, NAICS & both together')\n",
    "    agg_exp(c, ['cbg', 'date'], fname='exposure_by_cbg')\n",
    "    agg_exp(c, ['naics', 'date'], fname='exposure_by_naics')\n",
    "    agg_exp(c, ['cbg', 'naics', 'date'], fname='exposure_by_cbg_naics')\n",
    "#     print(f'time elapsed: {(time()-t)/60:.2f} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPS\n",
    "Regularized physical spacing (RPS) at the POI level"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def get_rps(city, write=False):\n",
    "    \"\"\"\n",
    "    Get POI-level regularized physical spacing (RPS) from hourly POI visits & POI area\n",
    "    \"\"\"\n",
    "    pat = city.pat\n",
    "    visits = city.poi_visits_hourly\n",
    "    idx = pat['includes_parking_lot'] == False\n",
    "    poi_ids = np.repeat(pat[idx]['poi_id'].values, 7)\n",
    "    areas = np.repeat(pat[idx]['area'].values, 7)\n",
    "    cbgs = np.repeat(pat[idx]['cbg'].values, 7)\n",
    "    dates = visits.index.rename('date')[np.repeat(idx.values, 7)]\n",
    "    hourly_mat = visits.values[np.repeat(idx.values, 7), :]\n",
    "    daily_visits = hourly_mat.sum(axis=1)\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        daily_rps = np.sqrt(hourly_mat).sum(1) * np.sqrt(areas) / daily_visits\n",
    "    city.rps = (pd.DataFrame(\n",
    "        {'date': dates, 'cbg': cbgs, 'poi_id': poi_ids,\n",
    "         'visits': daily_visits, 'rps': daily_rps, 'area': areas})\n",
    "                .set_index(['cbg', 'poi_id', 'date']))\n",
    "    if write:\n",
    "        city.rps.to_pickle(city.dir + '/rps.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PET\n",
    "Pessimistic exposure time (PET) at the POI level"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def get_pet(city, midpoints=dwell_points['new'], write=False):\n",
    "    \"\"\"\n",
    "    Get the POI-level daily pessimistic exposure time (PCT) of a city\n",
    "    from hourly POI visits and weekly dwell time distribution matrix.\n",
    "    \"\"\"\n",
    "    # get the weekly dwell time distribution matrix\n",
    "    distr = np.array(city.pat['dwell_bins'].tolist()).copy()\n",
    "    distr = distr / distr.sum(axis=1)[:, None]\n",
    "    distr[:, 3] = distr[:, 3] + distr[:, 4]\n",
    "    distr = distr[:, :4]\n",
    "    d1, d2, d3, d4 = distr.T\n",
    "    t1, t2, t3, t4 = midpoints\n",
    "\n",
    "    # compute the weekly alpha coefficient\n",
    "    alpha = (t1 * (d1 ** 2 + 2 * d1 * d2 + 2 * d1 * d3 + 2 * d1 * d4) +\n",
    "             t2 * (d2 ** 2 + 2 * d2 * d3 + 2 * d3 * d4) +\n",
    "             t3 * (d3 ** 2 + 2 * d3 * d4) +\n",
    "             t4 * (d4 ** 2))\n",
    "\n",
    "    # calculate the weekly beta coefficient\n",
    "    beta = distr @ midpoints\n",
    "\n",
    "    # get the hourly visit matrix\n",
    "    pat_mat = city.poi_visits_hourly.values.astype(np.int32)\n",
    "\n",
    "    # convert ones to zeros in this matrix because ones, like zeros, have no\n",
    "    # contribution in social contact; converting them to zero ensures that\n",
    "    # those ones do not contribute to the hourly social time becoming -ve\n",
    "    pat_mat = np.where(pat_mat == 1, 0, pat_mat)\n",
    "\n",
    "    # apply the main formula of contact time to get the daily averaged social\n",
    "    # time, keeping in mind that weekly vectors are cast to daily by repeating\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        pet_vals = np.repeat(alpha, 7) * ((pat_mat ** 2).sum(1) / (\n",
    "            pat_mat.sum(1))) - np.repeat(beta, 7)\n",
    "\n",
    "    # create the daily contact time dataframe\n",
    "    pet_df = pd.DataFrame({\n",
    "        'date': city.poi_visits_hourly.index,\n",
    "        'cbg': np.repeat(city.pat['cbg'].values, 7),\n",
    "        'poi_id': np.repeat(city.pat['poi_id'].values, 7),\n",
    "        'naics': np.repeat(city.pat['naics'].values, 7),\n",
    "        'visits': pat_mat.sum(axis=1),\n",
    "        'pet': pet_vals\n",
    "    }).set_index(['cbg', 'poi_id', 'date'])\n",
    "    # exclude nulls, negatives & other outlier category 'Other Airport Operations'\n",
    "    pet_df = (pet_df.query('pet >= 0')\n",
    "              .query('naics != 488119').drop(columns=['naics']))\n",
    "\n",
    "    if write:\n",
    "        pet_df.to_pickle(city.dir + '/pet.pickle')\n",
    "    \n",
    "    city.pet = pet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CEI\n",
    "CEI: Contact Exposure Index\n",
    "\n",
    "It is the hourly ratio of PET (minutes-persons) by the POI area (sq. ft.)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def get_pet_cei(city, write=False):\n",
    "    \"\"\"\n",
    "    Get the exposure index values at POI-day level from the already created\n",
    "    PET data.\n",
    "    \"\"\"\n",
    "    pois = (pd.read_pickle(f'{city.dir}/places.pickle')\n",
    "            .query('includes_parking_lot == False')\n",
    "            [['poi_id', 'area_sqft']]\n",
    "            .rename(columns={'area_sqft': 'area'})\n",
    "            .assign(area = lambda x: x['area']/1000))\n",
    "    pet_cei = (pd.read_pickle(f'{city.dir}/pet.pickle')\n",
    "               .reset_index()\n",
    "               .drop(columns=['area', 'ei'], errors='ignore')\n",
    "               .merge(pois, on='poi_id')\n",
    "               .assign(cei = lambda x: x['pet']/np.sqrt(x['area']))\n",
    "               .set_index(['cbg', 'poi_id', 'date']))\n",
    "    if write:\n",
    "        pd.to_pickle(pet_cei, f'{city.dir}/pet.pickle')\n",
    "    return pet_cei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def agg_rps_by_cbg(city):\n",
    "    \"\"\"Aggregate RPS by CBG as average of RPS weighted by visits per CBG\"\"\"\n",
    "    df = city.rps.dropna().assign(rps_x_visits = lambda x: x['visits']*x['rps'])\n",
    "    agg_df = (df.groupby(['cbg', 'date'])\n",
    "              [['visits', 'rps_x_visits', 'area']].sum()\n",
    "              .assign(rps = lambda x: x['rps_x_visits']/x['visits']))\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PET/CEI"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def agg_pet_by_cbg(city, write=False):\n",
    "    \"\"\"\n",
    "    Aggregate PET & EI by CBG as average of PET weighted by PET visits per CBG\n",
    "    \"\"\"\n",
    "    df = city.pet.dropna().assign(pet_x_visits = lambda x: x['visits']*x['pet'])\n",
    "    agg_df = (df.groupby(['cbg', 'date'])\n",
    "              [['visits', 'pet_x_visits']].sum()\n",
    "              .assign(pet = lambda x: x['pet_x_visits']/x['visits'])\n",
    "              .rename(columns={'visits': 'visits_pet'}))\n",
    "    \n",
    "    grp = (city.pet\n",
    "           .dropna()\n",
    "           .groupby(['cbg', 'date']))\n",
    "    visits = grp['visits'].sum().rename('visits_pet')\n",
    "    area = grp['area'].sum()\n",
    "    pet = (grp.apply(wtd_avg, val='pet', wt='visits').rename('pet'))\n",
    "    cei = (grp.apply(wtd_avg, val='cei', wt='visits').rename('cei'))\n",
    "    return pd.concat([visits, pet, area, cei], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def concat_rps_pet_by_cbg(city, write=True):\n",
    "    df = pd.concat([agg_rps_by_cbg(city), agg_pet_by_cbg(city)], axis=1)\n",
    "    if write:\n",
    "        df.to_pickle(f'{city.dir}/rps_pet_by_cbg.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate by industry"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def agg_exp_by_naics(city, write=False):\n",
    "    \"\"\"\n",
    "    Aggregate exposure metrics by industry (NAICS) as averages weighted by\n",
    "    visits per CBG.\n",
    "    \"\"\"\n",
    "    df = city.exp.dropna()\n",
    "    exp_vars = ['rps', 'pet', 'cei']\n",
    "    for var in exp_vars:\n",
    "        df[var+'_x_visits'] = df['visits'] * df[var]\n",
    "    agg_df = (df.groupby(['naics', 'date'])\n",
    "              [['visits'] + [x+'_x_visits' for x in exp_vars]].sum())\n",
    "    for var in exp_vars:\n",
    "        agg_df[var] = agg_df[var+'_x_visits'] / agg_df['visits']\n",
    "        del agg_df[var+'_x_visits']\n",
    "\n",
    "    if write:\n",
    "        agg_df.to_pickle(city.dir + '/exposure_by_naics.pickle')\n",
    "    \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def agg_rps_by_naics(city):\n",
    "    pois = city.pois[['poi_id', 'naics']]\n",
    "    rps_by_poi = (city.rps.reset_index()\n",
    "                  .merge(pois, on='poi_id')\n",
    "                  .drop(columns=['cbg','poi_id'])\n",
    "                  .dropna()\n",
    "                  .groupby(['naics', 'date']))\n",
    "    # now aggregate\n",
    "    visits = rps_by_poi['visits'].sum().rename('visits_rps')\n",
    "    wtd_rps = (rps_by_poi\n",
    "              .apply(lambda x: np.average(x['rps'], weights=x['visits']))\n",
    "              .rename('rps'))\n",
    "    return pd.concat([visits, wtd_rps], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PET/CEI"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def agg_pet_by_naics(city):\n",
    "    grp = (city.pet\n",
    "           .join(city.pois.set_index('poi_id')['naics'], how='inner')\n",
    "           .groupby(['naics', 'date']))\n",
    "    # now aggregate\n",
    "    visits = grp['visits'].sum().rename('visits_pet')\n",
    "    area = grp['area'].sum()\n",
    "    pet = grp.apply(wtd_avg, val='pet', wt='visits').rename('pet')\n",
    "    cei = grp.apply(wtd_avg, val='cei', wt='visits').rename('cei')\n",
    "    return pd.concat([visits, pet, area, cei], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def concat_rps_pet_by_naics(city, write=True):\n",
    "    df = pd.concat([agg_rps_by_naics(city), agg_pet_by_naics(city)], axis=1)\n",
    "    if write:\n",
    "        df.to_pickle(f'{city.dir}/rps_pet_by_naics.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate by CBG & industry"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def agg_exp_by_cbg_naics(city, write=False):\n",
    "    \"\"\"\n",
    "    Aggregate exposure metrics by CBG & industry (NAICS) as averages weighted\n",
    "    by visits per CBG.\n",
    "    \"\"\"\n",
    "    df = city.exp.dropna()\n",
    "    exp_vars = ['rps', 'pet', 'cei']\n",
    "    for var in exp_vars:\n",
    "        df[var+'_x_visits'] = df['visits'] * df[var]\n",
    "    agg_df = (df.groupby(['cbg', 'naics', 'date'])\n",
    "              [['visits'] + [x+'_x_visits' for x in exp_vars]].sum())\n",
    "    for var in exp_vars:\n",
    "        agg_df[var] = agg_df[var+'_x_visits'] / agg_df['visits']\n",
    "        del agg_df[var+'_x_visits']\n",
    "\n",
    "    if write:\n",
    "        agg_df.to_pickle(city.dir + '/exposure_by_naics.pickle')\n",
    "    \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "source": [
    "%%time\n",
    "exp_naics = agg_exp_by_naics(c, write=True)\n",
    "exp_naics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def agg_rps_by_cbg_naics(city):\n",
    "    grouped = (city.rps\n",
    "               .reset_index(['cbg', 'date'])\n",
    "               .join(city.pois.set_index('poi_id')['naics'])\n",
    "               .dropna()\n",
    "               .groupby(['cbg', 'naics', 'date']))\n",
    "    # now aggregate\n",
    "    visits = grouped['visits'].sum().rename('visits_rps')\n",
    "    wtd_rps = (grouped\n",
    "               .apply(lambda x: np.average(x['rps'], weights=x['visits']))\n",
    "               .rename('rps'))\n",
    "    return pd.concat([visits, wtd_rps], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PET/CEI"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def agg_pet_by_cbg_naics(city):\n",
    "    grp = (city.pet\n",
    "           .reset_index(['cbg', 'date'])\n",
    "           .join(city.pois.set_index('poi_id')['naics'])\n",
    "           .assign(petXvisits = lambda x: x['pet']*x['visits'])\n",
    "           .dropna()\n",
    "           .groupby(['cbg', 'naics', 'date']))\n",
    "    # now aggregate\n",
    "    visits = grp['visits'].sum().rename('visits_pet')\n",
    "    area = grp['area'].sum()\n",
    "    pet = grp.apply(wtd_avg, val='pet', wt='visits').rename('pet')\n",
    "    cei = grp.apply(wtd_avg, val='cei', wt='visits').rename('cei')\n",
    "    return pd.concat([visits, pet, area, cei], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def concat_rps_pet_by_cbg_naics(city, write=True):\n",
    "    df = pd.concat([agg_rps_by_cbg_naics(city), agg_pet_by_cbg_naics(city)], axis=1)\n",
    "    if write:\n",
    "        df.to_pickle(f'{city.dir}/rps_pet_by_cbg_naics.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
